{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9NQs9V6LKDE"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化环境\n",
        "env = gym.make('MountainCar-v0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H507vMjLiUv",
        "outputId": "f36e44ee-b653-44b2-ea62-64872bd471a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化 CartPole 环境\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# 设置分箱的参数\n",
        "num_buckets = (1, 1, 6, 3)  # 每个状态的分箱数量\n",
        "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "state_bounds[1] = [-0.5, 0.5]\n",
        "state_bounds[3] = [-np.radians(50), np.radians(50)]\n",
        "buckets = [np.linspace(state_bounds[i][0], state_bounds[i][1], num_buckets[i] - 1) for i in range(len(state_bounds))]\n",
        "\n",
        "# 离散化状态向量\n",
        "def discretize_state(state):\n",
        "    discrete_state = []\n",
        "    for i in range(len(state)):\n",
        "        discrete_state.append(np.digitize(state[i], buckets[i]))\n",
        "    return tuple(discrete_state)\n",
        "\n",
        "# 初始化 Q 表\n",
        "Q = np.zeros(num_buckets + (env.action_space.n,))\n",
        "\n",
        "# SARSA 算法\n",
        "def sarsa(env, Q, num_episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        discrete_state = discretize_state(state)\n",
        "        action = np.argmax(Q[discrete_state])\n",
        "        while True:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_discrete_state = discretize_state(next_state)\n",
        "            next_action = np.argmax(Q[next_discrete_state])\n",
        "            target = reward + gamma * Q[next_discrete_state][next_action]\n",
        "            Q[discrete_state][action] += alpha * (target - Q[discrete_state][action])\n",
        "            state = next_state\n",
        "            discrete_state = next_discrete_state\n",
        "            action = next_action\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# 运行 SARSA 算法\n",
        "sarsa(env, Q)\n",
        "\n",
        "# 测试最终策略\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "while True:\n",
        "    env.render()\n",
        "    discrete_state = discretize_state(state)\n",
        "    action = np.argmax(Q[discrete_state])\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Total reward:\", total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2XeKfu1LlW0",
        "outputId": "06bb5bad-7fdd-4f0e-eeff-394bcf1e1b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 203.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化 CartPole 环境\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# 设置分箱的参数\n",
        "num_buckets = (1, 1, 6, 3)  # 每个状态的分箱数量\n",
        "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "state_bounds[1] = [-0.5, 0.5]\n",
        "state_bounds[3] = [-np.radians(50), np.radians(50)]\n",
        "buckets = [np.linspace(state_bounds[i][0], state_bounds[i][1], num_buckets[i] - 1) for i in range(len(state_bounds))]\n",
        "\n",
        "# 离散化状态向量\n",
        "def discretize_state(state):\n",
        "    discrete_state = []\n",
        "    for i in range(len(state)):\n",
        "        discrete_state.append(np.digitize(state[i], buckets[i]))\n",
        "    return tuple(discrete_state)\n",
        "\n",
        "# 初始化 Q 表\n",
        "Q = np.zeros(num_buckets + (env.action_space.n,))\n",
        "\n",
        "# Q-learning 算法\n",
        "def q_learning(env, Q, num_episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        discrete_state = discretize_state(state)\n",
        "        while True:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[discrete_state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_discrete_state = discretize_state(next_state)\n",
        "            target = reward + gamma * np.max(Q[next_discrete_state])\n",
        "            Q[discrete_state][action] += alpha * (target - Q[discrete_state][action])\n",
        "            discrete_state = next_discrete_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# 运行 Q-learning 算法\n",
        "q_learning(env, Q)\n",
        "\n",
        "# 测试最终策略\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "while True:\n",
        "    env.render()\n",
        "    discrete_state = discretize_state(state)\n",
        "    action = np.argmax(Q[discrete_state])\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Total reward:\", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b19y9LCLnLl",
        "outputId": "a4648a99-3098-4a2f-bee3-481dc315c577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化 CartPole 环境\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# 设置分箱的参数\n",
        "num_buckets = (1, 1, 6, 3)  # 每个状态的分箱数量\n",
        "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "state_bounds[1] = [-0.5, 0.5]\n",
        "state_bounds[3] = [-np.radians(50), np.radians(50)]\n",
        "buckets = [np.linspace(state_bounds[i][0], state_bounds[i][1], num_buckets[i] - 1) for i in range(len(state_bounds))]\n",
        "\n",
        "# 离散化状态向量\n",
        "def discretize_state(state):\n",
        "    discrete_state = []\n",
        "    for i in range(len(state)):\n",
        "        discrete_state.append(np.digitize(state[i], buckets[i]))\n",
        "    return tuple(discrete_state)\n",
        "\n",
        "# 初始化 Q 表\n",
        "Q1 = np.zeros(num_buckets + (env.action_space.n,))\n",
        "Q2 = np.zeros(num_buckets + (env.action_space.n,))\n",
        "\n",
        "# Double Q-learning 算法\n",
        "def double_q_learning(env, Q1, Q2, num_episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        discrete_state = discretize_state(state)\n",
        "        while True:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q1[discrete_state] + Q2[discrete_state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_discrete_state = discretize_state(next_state)\n",
        "            if np.random.rand() < 0.5:\n",
        "                next_action = np.argmax(Q1[next_discrete_state])\n",
        "                target = reward + gamma * Q2[next_discrete_state][next_action]\n",
        "                Q1[discrete_state][action] += alpha * (target - Q1[discrete_state][action])\n",
        "            else:\n",
        "                next_action = np.argmax(Q2[next_discrete_state])\n",
        "                target = reward + gamma * Q1[next_discrete_state][next_action]\n",
        "                Q2[discrete_state][action] += alpha * (target - Q2[discrete_state][action])\n",
        "            discrete_state = next_discrete_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# 运行 Double Q-learning 算法\n",
        "double_q_learning(env, Q1, Q2)\n",
        "\n",
        "# 测试最终策略\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "while True:\n",
        "    env.render()\n",
        "    discrete_state = discretize_state(state)\n",
        "    action = np.argmax(Q1[discrete_state] + Q2[discrete_state])\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Total reward:\", total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5xtdG24LolK",
        "outputId": "153b326d-95fb-4a1a-8b9b-2f523bbc7a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 16.0\n"
          ]
        }
      ]
    }
  ]
}