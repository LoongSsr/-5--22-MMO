{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7fJLi2AORn9"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, actor_critic, optimizer, gamma=0.99, max_episodes=1000):\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        # 在每个episode开始时创建一个新的GradientTape\n",
        "        with tf.GradientTape() as tape:\n",
        "            while True:\n",
        "                state_tensor = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
        "                logits, value = actor_critic(state_tensor)\n",
        "                action_probs = tf.nn.softmax(logits)\n",
        "                action = np.random.choice(env.action_space.n, p=np.squeeze(action_probs.numpy()))\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                episode_reward += reward\n",
        "\n",
        "                next_state_tensor = tf.expand_dims(tf.convert_to_tensor(next_state), 0)\n",
        "                _, next_value = actor_critic(next_state_tensor)\n",
        "                td_target = reward + gamma * next_value * (1 - done)\n",
        "                td_error = td_target - value\n",
        "\n",
        "                # 计算actor和critic损失\n",
        "                action_prob = tf.gather(action_probs[0], action)\n",
        "                actor_loss = -tf.math.log(action_prob) * td_error\n",
        "                critic_loss = td_error ** 2\n",
        "                total_loss = actor_loss + critic_loss\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "                else:\n",
        "                    state = next_state\n",
        "\n",
        "        # 计算梯度并更新参数\n",
        "        grads = tape.gradient(total_loss, actor_critic.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, actor_critic.trainable_variables))\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(\"Episode {}: Total Reward = {}\".format(episode, episode_reward))\n",
        "\n",
        "# 初始化环境和Actor-Critic模型\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n\n",
        "actor_critic = ActorCritic(num_actions)\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "# 训练Actor-Critic模型\n",
        "train(env, actor_critic, optimizer)"
      ],
      "metadata": {
        "id": "v_3YpSEjPPc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc3d0f4-58d1-46a8-c4ee-c7547a9f1c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward = 31.0\n",
            "Episode 10: Total Reward = 37.0\n",
            "Episode 20: Total Reward = 33.0\n",
            "Episode 30: Total Reward = 33.0\n",
            "Episode 40: Total Reward = 19.0\n",
            "Episode 50: Total Reward = 11.0\n",
            "Episode 60: Total Reward = 9.0\n",
            "Episode 70: Total Reward = 14.0\n",
            "Episode 80: Total Reward = 19.0\n",
            "Episode 90: Total Reward = 36.0\n",
            "Episode 100: Total Reward = 18.0\n",
            "Episode 110: Total Reward = 11.0\n",
            "Episode 120: Total Reward = 14.0\n",
            "Episode 130: Total Reward = 13.0\n",
            "Episode 140: Total Reward = 11.0\n",
            "Episode 150: Total Reward = 12.0\n",
            "Episode 160: Total Reward = 10.0\n",
            "Episode 170: Total Reward = 14.0\n",
            "Episode 180: Total Reward = 9.0\n",
            "Episode 190: Total Reward = 11.0\n",
            "Episode 200: Total Reward = 13.0\n",
            "Episode 210: Total Reward = 17.0\n",
            "Episode 220: Total Reward = 11.0\n",
            "Episode 230: Total Reward = 18.0\n",
            "Episode 240: Total Reward = 9.0\n",
            "Episode 250: Total Reward = 15.0\n",
            "Episode 260: Total Reward = 16.0\n",
            "Episode 270: Total Reward = 10.0\n",
            "Episode 280: Total Reward = 11.0\n",
            "Episode 290: Total Reward = 9.0\n",
            "Episode 300: Total Reward = 16.0\n",
            "Episode 310: Total Reward = 14.0\n",
            "Episode 320: Total Reward = 8.0\n",
            "Episode 330: Total Reward = 12.0\n",
            "Episode 340: Total Reward = 21.0\n",
            "Episode 350: Total Reward = 16.0\n",
            "Episode 360: Total Reward = 10.0\n",
            "Episode 370: Total Reward = 8.0\n",
            "Episode 380: Total Reward = 12.0\n",
            "Episode 390: Total Reward = 14.0\n",
            "Episode 400: Total Reward = 20.0\n",
            "Episode 410: Total Reward = 12.0\n",
            "Episode 420: Total Reward = 10.0\n",
            "Episode 430: Total Reward = 13.0\n",
            "Episode 440: Total Reward = 10.0\n",
            "Episode 450: Total Reward = 15.0\n",
            "Episode 460: Total Reward = 13.0\n",
            "Episode 470: Total Reward = 23.0\n",
            "Episode 480: Total Reward = 21.0\n",
            "Episode 490: Total Reward = 14.0\n",
            "Episode 500: Total Reward = 10.0\n",
            "Episode 510: Total Reward = 20.0\n",
            "Episode 520: Total Reward = 21.0\n",
            "Episode 530: Total Reward = 23.0\n",
            "Episode 540: Total Reward = 18.0\n",
            "Episode 550: Total Reward = 10.0\n",
            "Episode 560: Total Reward = 13.0\n",
            "Episode 570: Total Reward = 9.0\n",
            "Episode 580: Total Reward = 18.0\n",
            "Episode 590: Total Reward = 16.0\n",
            "Episode 600: Total Reward = 10.0\n",
            "Episode 610: Total Reward = 9.0\n",
            "Episode 620: Total Reward = 12.0\n",
            "Episode 630: Total Reward = 19.0\n",
            "Episode 640: Total Reward = 13.0\n",
            "Episode 650: Total Reward = 12.0\n",
            "Episode 660: Total Reward = 10.0\n",
            "Episode 670: Total Reward = 14.0\n",
            "Episode 680: Total Reward = 11.0\n",
            "Episode 690: Total Reward = 9.0\n",
            "Episode 700: Total Reward = 12.0\n",
            "Episode 710: Total Reward = 14.0\n",
            "Episode 720: Total Reward = 8.0\n",
            "Episode 730: Total Reward = 12.0\n",
            "Episode 740: Total Reward = 9.0\n",
            "Episode 750: Total Reward = 10.0\n",
            "Episode 760: Total Reward = 12.0\n",
            "Episode 770: Total Reward = 8.0\n",
            "Episode 780: Total Reward = 10.0\n",
            "Episode 790: Total Reward = 9.0\n",
            "Episode 800: Total Reward = 11.0\n",
            "Episode 810: Total Reward = 10.0\n",
            "Episode 820: Total Reward = 9.0\n",
            "Episode 830: Total Reward = 8.0\n",
            "Episode 840: Total Reward = 10.0\n",
            "Episode 850: Total Reward = 10.0\n",
            "Episode 860: Total Reward = 11.0\n",
            "Episode 870: Total Reward = 9.0\n",
            "Episode 880: Total Reward = 9.0\n",
            "Episode 890: Total Reward = 9.0\n",
            "Episode 900: Total Reward = 12.0\n",
            "Episode 910: Total Reward = 10.0\n",
            "Episode 920: Total Reward = 11.0\n",
            "Episode 930: Total Reward = 11.0\n",
            "Episode 940: Total Reward = 10.0\n",
            "Episode 950: Total Reward = 9.0\n",
            "Episode 960: Total Reward = 12.0\n",
            "Episode 970: Total Reward = 9.0\n",
            "Episode 980: Total Reward = 10.0\n",
            "Episode 990: Total Reward = 8.0\n"
          ]
        }
      ]
    }
  ]
}